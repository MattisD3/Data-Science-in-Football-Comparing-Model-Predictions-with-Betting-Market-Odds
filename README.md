# Data Science in Football: Comparing Model Predictions with Betting Market Odds

## Research Question
To what extent can football performance metrics such as expected goals (xG), shots, possession, and Elo ratings be used to predict match outcome probabilities, and how do these model-implied probabilities differ from bookmaker odds?

## Setup
# Create environment:
conda env create -f environment.yml
conda activate football-outcome-prediction

# Usage
Run full pipeline:
python main.py

This executes the entire workflow:
-   data cleaning and merging
-   feature engineering (rolling xG, points form, Elo ratings…)
-   training Logistic Regression, Random Forest, and XGBoost
-   probability calibration (Platt, Isotonic)
-   comparison of model probabilities vs bookmaker implied odds
-   generation of plots + summary metrics in the results/ folder

## Project Structure
```
DATA-SCIENCE-IN-FOOTBALL-COMPARING-MODEL-PREDICTIONS-WITH-BETTING-MARKET-ODDS/
├── README.md                     # Overview + installation + usage instructions
├── requirements.txt              
├── environment.yml               # Conda environment file for reproducibility
├── project_report.md             # Final project report (Markdown version)
├── PROPOSAL                      # Initial project proposal
├── main.py                             # Main script to run the full pipeline end-to-end
├── data/                               # Input datasets
│   ├── raw/                            # Raw EPL 22/23 data (Football-Data, FBref)
|       ├── matches.csv                 
│       └── data_bookmakers_22_23.csv
│   └── processed/                      # Cleaned and merged datasets used for modeling
        ├── clean_bookmakers_22_23.csv
        ├── clean_matches_22_23.csv
        ├── elo_ratings_22_23.csv
        ├── features_matches_long_22_23.csv
        ├── features_matches_long_elo_22_23.csv
        ├── matches_long_22_23.csv
        └── matches_wides_22_23.csv
│
├── notebooks/                          # Jupyter notebooks for exploration and prototyping
│
├── src/                                # Main source code for the pipeline
│   ├── A_clean_matches.py                      # Cleaning and preprocessing match data
│   ├── B_clean_bookmakers_data.py              # Cleaning bookmakers' odds
│   ├── C_merge_data.py                         # Merging all data sources together
│   ├── D_feature_engineering.py                # Feature creation (xG rolling stats, etc.)
│   ├── E_model_baseline_logistic.py            # Logistic regression baseline model
|   ├── F_elo_rating.py                         # Creating Elo-based features
|   ├── G_add_elo_to_long_features.py           # Merging Elo-based features to the feature dataset
|   ├── H_model_baseline_logistic_with_elo.py   # Logistic regression including Elo features
│   ├── I_random_forest.py                      # Random Forest model
│   ├── J_xgboost.py                            # XGBoost model
│   ├── K_calibration_experiments.py            # Calibration (Platt, Isotonic) + curves
│   └── L_model_vs_bookmakers.py                # Model vs bookmakers probability comparison
│
├── results/                                    # Outputs generated by the pipeline (Plots, metrics, calibration curves)
    ├── big_six_vs_others_away_bias_22_23.png
    ├── big_six_vs_others_home_bias_22_23.png
    ├── calibration_model_vs_book_22_23.png
    ├── calibration_plot_baseline.png
    ├── calibration_plot_best.png
    ├── calibration_plot_elo.png
    ├── calibration_plot_random_forest.png
    ├── calibration_plot_xgboost.png
    ├── calibration_summary_22_23.csv
    ├── errors_model_vs_book_22_23.png
    ├── feature_importance_xgb_raw_22_23.png
    ├── global_bias_bar_22_23.png
    ├── model_vs_bookmakers_22_23.csv
    ├── prob_dist_model_vs_book_22_23.png
    ├── scatter_model_vs_book_22_23.png
    ├── team_away_bias_bar_22_23.png
    ├── team_away_bias_heatmap_22_23.png
    ├── team_home_bias_bar_22_23.png
    └── team_home_bias_heatmap_22_23.png
|
├── tests/                       
├── docs/                         
└── .gitignore                                  # Files and folders excluded from version control

```

## Key Results
Model performance (EPL 22/23):

| Model + calibration     | Accuracy | Log-loss | Mean-Brier | 
|-------------------------|----------|----------|------------|
| Logistic + Isotonic     | 0.60714  | 1.01539  | 0.19339    |
| Logistic + Platt        | 0.61607  | 0.93570  | 0.18418    | 
| Logistic + raw          | 0.60714  | 0.94864  | 0.18629    |
| RandomForest + Isotonic | 0.61607  | 1.83101  | 0.18019    | 
| RandomForest + Platt    | 0.61607  | 0.91112  | 0.17709    |
| RandomForest + raw      | 0.62500  | 0.91111  | 0.17707    |
| XGBoost + Isotonic      | 0.60714  | 1.55443  | 0.17983    |
| XGBoost + Platt         | 0.63393  | 0.90839  | 0.17551    |
| **XGBoost + raw**       | 0.61607  | 0.90774  | 0.17550    |

| Outcome | RMSE (model vs book)| MAE (model vs book) |
|---------|---------------------|---------------------|
| H       | 0.20713             | 0.16992             |
| D       | 0.16699             | 0.13507             |
| A       | 0.17963             | 0.14796             |

Insights:
Bookmakers remain better calibrated overall.
The model performs surprisingly well for away-win probabilities.
Bias analysis shows the model underestimates Chelsea and Manchester City at home, partly due to bookmaker boosting and reputation effects.

## Dependencies
All included in environment.yml (Python 3.11, pandas, numpy, scikit-learn, xgboost, matplotlib, seaborn, jupyter).
Recreate environment with:
conda env create -f environment.yml
conda activate football-outcome-prediction

## Reproducibility
To regenerate all plots and evaluation files:
python main.py

Everything will be saved inside the results/ folder.