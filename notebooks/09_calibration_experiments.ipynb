{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1f5b88",
   "metadata": {},
   "source": [
    "# 09 - Calibration Platt Isotonic\n",
    "\n",
    "Goals:\n",
    "- Load the data `features_match_long_elo_22_23`\n",
    "- Choose X (features) and y (target = result H/D/A)\n",
    "- Split train/test by time order\n",
    "- Train my 3 model :\n",
    "    - Logistic regression\n",
    "    - Random Forest\n",
    "    - XGBoost\n",
    "- Apply :\n",
    "    - No calibration (raw)\n",
    "    - Platt scaling (sigmoid)\n",
    "    - Isotonic regression\n",
    "- For each, calculate :\n",
    "    - Accuracy\n",
    "    - Log Loss\n",
    "    - Brier Score\n",
    "- Compare all results in a single summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "# Load long-format features with Elo (two rows per match, one per team)\n",
    "df = pd.read_csv(FEATURES_ELO_PATH)\n",
    "print(\"Long + Elo features shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cab46fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"result\"\n",
    "\n",
    "feature_cols = [\n",
    "    \"is_home\",\n",
    "    \"rolling_xg_for_5\",\n",
    "    \"rolling_xg_against_5\",\n",
    "    \"rolling_xg_diff_5\",\n",
    "    \"rolling_points_5\",\n",
    "    \"strength_points_5\",\n",
    "    \"elo_team_before\",\n",
    "    \"elo_diff_for_team\",\n",
    "]\n",
    "\n",
    "# Check NaNs before dropping\n",
    "print(\"NaN per column BEFORE drop:\", df.shape)\n",
    "print(df[feature_cols + [target_col]].isna().sum())\n",
    "\n",
    "# Drop rows with missing values in features or target\n",
    "df_clean = df.dropna(subset=feature_cols + [target_col]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nShape AFTER drop:\", df_clean.shape)\n",
    "print(\"NaN per column AFTER drop:\")\n",
    "print(df_clean[feature_cols + [target_col]].isna().sum())\n",
    "\n",
    "# Ensure date is datetime and sort chronologically\n",
    "df_clean[\"date\"] = pd.to_datetime(df_clean[\"date\"])\n",
    "df_clean = df_clean.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "X = df_clean[feature_cols].copy()\n",
    "y = df_clean[target_col].copy()\n",
    "\n",
    "print(\"\\nX shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3749ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "n = len(df_clean)\n",
    "train_size = int(train_ratio * n)\n",
    "\n",
    "X_train = X.iloc[:train_size]\n",
    "X_test  = X.iloc[train_size:]\n",
    "y_train = y.iloc[:train_size]\n",
    "y_test  = y.iloc[train_size:]\n",
    "\n",
    "print(\"Train shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shapes :\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Encode target labels (H/D/A) into integers\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc  = le.transform(y_test)\n",
    "\n",
    "print(\"\\nClasses:\", le.classes_)     # expected ['A', 'D', 'H']\n",
    "print(\"First 10 encoded y_train:\", y_train_enc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95183616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for logistic regression only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "print(\"Scaled shapes:\", X_train_scaled.shape, X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy, log loss and Brier scores for given probabilities.\n",
    "def evaluate_probas(y_test_enc: np.ndarray, y_proba: np.ndarray, le: LabelEncoder, label: str = \"\") -> dict:\n",
    "    # Predicted classes from probabilities\n",
    "    y_pred_enc = np.argmax(y_proba, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_enc, y_pred_enc)\n",
    "    ll = log_loss(y_test_enc, y_proba)\n",
    "\n",
    "    # Brier scores per class\n",
    "    brier_scores = []\n",
    "    for class_idx, class_label in enumerate(le.classes_):\n",
    "        y_true_binary = (y_test_enc == class_idx).astype(int)\n",
    "        y_prob_class = y_proba[:, class_idx]\n",
    "        brier = brier_score_loss(y_true_binary, y_prob_class)\n",
    "        brier_scores.append(brier)\n",
    "\n",
    "    mean_brier = float(np.mean(brier_scores))\n",
    "\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    print(f\"Accuracy : {accuracy:.3f}\")\n",
    "    print(f\"Log loss : {ll:.4f}\")\n",
    "    print(f\"Mean Brier score: {mean_brier:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"log_loss\": float(ll),\n",
    "        \"brier_scores\": brier_scores,\n",
    "        \"mean_brier\": mean_brier,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base logistic regression model (multinomial)\n",
    "logit = LogisticRegression(\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "logit.fit(X_train_scaled, y_train_enc)\n",
    "y_proba_logit_raw = logit.predict_proba(X_test_scaled)\n",
    "\n",
    "metrics_logit_raw = evaluate_probas(\n",
    "    y_test_enc,\n",
    "    y_proba_logit_raw,\n",
    "    le,\n",
    "    label=\"Logistic (raw, uncalibrated)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47695c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with Platt scaling (sigmoid)\n",
    "logit_platt = CalibratedClassifierCV(\n",
    "    estimator=LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    method=\"sigmoid\",   # Platt scaling\n",
    "    cv=3,               # 3-fold CV on training set\n",
    ")\n",
    "\n",
    "logit_platt.fit(X_train_scaled, y_train_enc)\n",
    "y_proba_logit_platt = logit_platt.predict_proba(X_test_scaled)\n",
    "\n",
    "metrics_logit_platt = evaluate_probas(\n",
    "    y_test_enc,\n",
    "    y_proba_logit_platt,\n",
    "    le,\n",
    "    label=\"Logistic + Platt (sigmoid)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cba4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with isotonic calibration\n",
    "logit_iso = CalibratedClassifierCV(\n",
    "    estimator=LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    method=\"isotonic\",\n",
    "    cv=3,\n",
    ")\n",
    "\n",
    "logit_iso.fit(X_train_scaled, y_train_enc)\n",
    "y_proba_logit_iso = logit_iso.predict_proba(X_test_scaled)\n",
    "\n",
    "metrics_logit_iso = evaluate_probas(\n",
    "    y_test_enc,\n",
    "    y_proba_logit_iso,\n",
    "    le,\n",
    "    label=\"Logistic + Isotonic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b1661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Random Forest model (uncalibrated)\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train_enc)\n",
    "y_proba_rf_raw = rf.predict_proba(X_test)\n",
    "\n",
    "metrics_rf_raw = evaluate_probas(\n",
    "    y_test_enc,\n",
    "    y_proba_rf_raw,\n",
    "    le,\n",
    "    label=\"Random Forest (raw, uncalibrated)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with Platt scaling\n",
    "rf_platt = CalibratedClassifierCV(\n",
    "    estimator=RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    method=\"sigmoid\",\n",
    "    cv=3,\n",
    ")\n",
    "\n",
    "rf_platt.fit(X_train, y_train_enc)\n",
    "y_proba_rf_platt = rf_platt.predict_proba(X_test)\n",
    "\n",
    "metrics_rf_platt = evaluate_probas(\n",
    "    y_test_enc,\n",
    "    y_proba_rf_platt,\n",
    "    le,\n",
    "    label=\"Random Forest + Platt (sigmoid)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with isotonic calibration\n",
    "rf_iso = CalibratedClassifierCV(\n",
    "    estimator=RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    method=\"isotonic\",\n",
    "    cv=3,\n",
    ")\n",
    "\n",
    "rf_iso.fit(X_train, y_train_enc)\n",
    "y_proba_rf_iso = rf_iso.predict_proba(X_test)\n",
    "\n",
    "metrics_rf_iso = evaluate_probas(\n",
    "    y_test_enc,\n",
    "    y_proba_rf_iso,\n",
    "    le,\n",
    "    label=\"Random Forest + Isotonic\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f358c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base XGBoost model (uncalibrated)\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=3,\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"mlogloss\",\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train_enc)\n",
    "y_proba_xgb_raw = xgb_model.predict_proba(X_test)\n",
    "\n",
    "metrics_xgb_raw = evaluate_probas(\n",
    "    y_test_enc,\n",
    "    y_proba_xgb_raw,\n",
    "    le,\n",
    "    label=\"XGBoost (raw, uncalibrated)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd2503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with Platt scaling\n",
    "xgb_platt = CalibratedClassifierCV(\n",
    "    estimator=xgb.XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=3,\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"mlogloss\",\n",
    "    ),\n",
    "    method=\"sigmoid\",\n",
    "    cv=3,\n",
    ")\n",
    "\n",
    "xgb_platt.fit(X_train, y_train_enc)\n",
    "y_proba_xgb_platt = xgb_platt.predict_proba(X_test)\n",
    "\n",
    "metrics_xgb_platt = evaluate_probas(\n",
    "    y_test_enc,\n",
    "    y_proba_xgb_platt,\n",
    "    le,\n",
    "    label=\"XGBoost + Platt (sigmoid)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef94dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with isotonic calibration\n",
    "xgb_iso = CalibratedClassifierCV(\n",
    "    estimator=xgb.XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=3,\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"mlogloss\",\n",
    "    ),\n",
    "    method=\"isotonic\",\n",
    "    cv=3,\n",
    ")\n",
    "\n",
    "xgb_iso.fit(X_train, y_train_enc)\n",
    "y_proba_xgb_iso = xgb_iso.predict_proba(X_test)\n",
    "\n",
    "metrics_xgb_iso = evaluate_probas(\n",
    "    y_test_enc,\n",
    "    y_proba_xgb_iso,\n",
    "    le,\n",
    "    label=\"XGBoost + Isotonic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4cb917",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "\n",
    "def add_row(model_name, calib_type, metrics):\n",
    "    summary.append({\n",
    "        \"model\": model_name,\n",
    "        \"calibration\": calib_type,\n",
    "        \"accuracy\": metrics[\"accuracy\"],\n",
    "        \"log_loss\": metrics[\"log_loss\"],\n",
    "        \"mean_brier\": metrics[\"mean_brier\"],\n",
    "    })\n",
    "\n",
    "add_row(\"Logistic\", \"raw\",      metrics_logit_raw)\n",
    "add_row(\"Logistic\", \"Platt\",    metrics_logit_platt)\n",
    "add_row(\"Logistic\", \"Isotonic\", metrics_logit_iso)\n",
    "\n",
    "add_row(\"RandomForest\", \"raw\",      metrics_rf_raw)\n",
    "add_row(\"RandomForest\", \"Platt\",    metrics_rf_platt)\n",
    "add_row(\"RandomForest\", \"Isotonic\", metrics_rf_iso)\n",
    "\n",
    "add_row(\"XGBoost\", \"raw\",      metrics_xgb_raw)\n",
    "add_row(\"XGBoost\", \"Platt\",    metrics_xgb_platt)\n",
    "add_row(\"XGBoost\", \"Isotonic\", metrics_xgb_iso)\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "df_summary.sort_values([\"model\", \"log_loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
